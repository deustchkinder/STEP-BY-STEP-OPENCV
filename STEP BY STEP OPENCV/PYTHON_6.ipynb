{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bd56b5-80c0-42a3-a67c-fae9031221fa",
   "metadata": {},
   "source": [
    "**MAKİNA ÖĞRENMESİ & DERİN ÖĞRENME**\n",
    "\n",
    "Makina Öğrenmesi:Eğitim verileri kullanılarak eğitilen bir modelin sonraki karşılaştığı durumlardan çıkarımlar yapılmasıdır.Herhangi bir açık talimat gerekmeksizin kendi başına görevleri gerçekleştirmesinin sağlanmasıdır.\n",
    "\n",
    "1. Denetimli Öğrenme:Verilerdeki bilinen(etiketli) özelliklere dayalı olarak sonucu tahmin etmeye çalışan bir makina öğrenimini algoritmasıdır.Bir girişe karşı sonucunun tahminini(gelecek tahmini) yapmak için geçmiş tecrübelerden öğrendiği tarihsel verileri kullanma işlemidir.Çok fazla etiketlenmiş veriye sahip olmamız gerektiği için veri toplama kısmında zorluklar geçirebiliriz.\n",
    "\n",
    "2. Denetimsiz Öğrenme:Öğrenilecek geçmiş etiketli verilere sahip değildir, bunun yerine anlık yapılarına, birbirlerine göre durumlarına ve desenlerine bakılarak öğrenilir.Bir tahmin yapabilmek için benzer özelliklere sahip verileri yapılarına, değerlerine ve etiketlerine göre gruplandırılmasıdır.Etiketlenmiş veriye çok fazla ihtiyaç duymaması modeli eğitmede bize avantaj sağlayabilir.\n",
    "\n",
    "3. Yarı Denetimli Öğrenme:Eğitilmesi için az miktarda etiketlenmiş veri,çok miktarda da etiketlenmemiş veri kullanılır.Bu sayede denetimli öğrenmenin etiketsiz verilerle çalışma avantajını birleştirilmiş olacaktır.Sonuçta az etiketlenmiş verinin ve çok etiketlenmemiş verilerin olduğu veri setleri için uygundur.\n",
    "\n",
    "4. Pekiştirmeli Öğrenme:Denetimli öğrenmedeki gibi etiketlenmiş verilere dayanmadığı gibi denetimsiz öğrenmenin aksine sonuca ulaşmak için ödül sistemi kullanılmaktadır.Bu modeli eğitmek için hazır bir veriden ziyade anlık olarak veri akışı kullanılmakta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414a5cc-48ca-41d7-aa62-51ec99d7d33c",
   "metadata": {},
   "source": [
    "**1-)YAPAY ZEKA**\n",
    "\n",
    "**İNSAN GİBİ DÜŞÜNEN SİSTEMLER**. İnsan zekasının bilgisayarlar tarafından taklit edilmesinin sağlanması.\n",
    "\n",
    "İnsan sinir sistemi milyarlarca sinir hücresi veya diğer ismi ile nöronlardan oluşmaktadır.Her bir nöron başka binlerce nöronlarla bağlantılı olup vazifesi elektriksel sinyaller ile bilgi transferi gerçekleştirmektedir. Dışarıdan gelen her bilgi girişi(ınput) elektriksel sinyal olarak dentritler vasıtasıyla sinir hücrelerine gelir ve burada birtakım işlemlerden geçtikten sonra tek bir elektriksel sinyal olarak aksonlar vasıtasıyla dışarıya aktarılır.\n",
    "\n",
    "**GİRİŞLER:X0..Xn - AĞIRLIKLAR:Wi1...Win -- TOPLAMA: X1.Wi1 + ....+ Xn.Win - AKTİVASYON FONKSİYONU: f -- ÇIKIŞ**\n",
    "Bu yapıda dışarıdan gelen sinyaller x ile, her bir sinyalinin ağırlığı veya etki derecesi w ile gösterilmiştir.Sonucun 0 çıkmasına karşın toplama fonksiyonuna b(bias)'da eklenebilmekte.\n",
    "\n",
    "Bir çok nöron hücresinin bir araya gelmesiyle oluşturulmuş sinir hücrelerine YAPAY SİNİR AĞLARI denilmekte.Bu yapıda her bir sütun bir katman olarak isimlendirir.İlk katmanda GİRİŞ DEĞERLERİ(X),en son katmanda ÇIKIŞ DEĞERLERİ(y) arasında yer alan diğer katmanlar ise HİDDEN LAYERS olarak literatürde geçmektedir.Bu ara katmanlar sadece kendilerinden bir önceki veya bir sonraki katman ile ilgilenirler.Eğer giriş katmanı ile çıkış katmanı arasında 3 veya daha fazla katman varsa bu ağ DERİN YAPAY SİNİR AĞI.Derin yapay sinir ağları ile yapılacak öğrenme modeline ise DERİN ÖĞRENME denilir.\n",
    "\n",
    "Aktivasyon Fonksiyonu:Öncelikle her bir katmanda aktivasyon fonksiyonlarına ihtiyaç duyulması zorunludur.Ağırlıklar(X0)'a göre işlem görmesiyle elde edilen sonucu bir düğümden veya ağ katmanındaki düğümlerden bir çıktıya nasıl dönüştürüleceğini tanımlamak için kurulur.\n",
    "\n",
    "1. Sigmoid:Elde edilecek sonuç 0 ile 1 arasındadır ve hesaplamaları çok yorucudur.\n",
    "2. Hiperbolik Tanjant:Elde edilecek sonuç -1 ile 1 arasındadır yine hesaplamaları çok yorucudur.\n",
    "3. RELU:Eğer sonuç 0'dan küçük ise sonuç 0 olacaktır lakin sonuç 0'dan büyük ise ağırlklara göre işlem görmesiyle elde edilen toplama fonksiyonun sonucu olacaktır. Ölü RELU (w>0 & x<0)\n",
    "4. MaxOut: Toplama fonksiyonu değerlerini oluşturan girişlerden hangisinden en büyük sonuç elde ediliyorsa çıkışa aktarılır. \n",
    "\n",
    "**Bir Yapay Zeka modelinin oluşturulmasında en önemli başlık modelin eğitilmesidir.Bilinmesi gerekenler;**\n",
    "1. Başarı Ölçütleri: Geliştirilen modelin ne kadar başarılı olup olmadığının matematiksel ölçütlemesidir.Test verisinden alınan yanıt ile ölçülebilir.Çünkü test verisinin olması gereken sonuçlar ile modelden elde edilmiş sonuçların karşılaştırılmasıdır.\n",
    "\n",
    "Bu karşılaştırma için KARMAŞIKLIK MATRİSİ kullanılır.\n",
    "1. TP:Gerçek değeri 1 ve tahmin edilen değer 1. \n",
    "2. TN:Gerçek değeri 0 ve tahmin edilen değer 0. \n",
    "3. FP:Gerçek değeri 0 fakat tahmin edilen değer 1. \n",
    "4. FN:Gerçek değeri 1 fakat tahmin edilen değer 0'dır. \n",
    "Sadece yapılan bir tahminin doğru/hatalı olduğunun bilinmesi gerçek sonucu yansıtmadığı için Karmaşıklık Matrisi oldukça detaylıdır. \n",
    "\n",
    "Karmaşıklık Matrisine göre elde edilen başarı ölçütleri: \n",
    "1. Accuracy(Doğruluk):Eğer dengeli dağıtılmış sınıflar var ise modelde tahmin ettiğimiz alanların toplam veri kümesine oranı ile hesaplanır. \n",
    "2. Specificity(Özgüllük):Sınıflandırıcının ne kadar gerçek negatif değeri doğru tahmin ettiğinin bir ölçüsüdür.\n",
    "3. Recall(Duyarlılık):Sınıflandırıcının ne kadar gerçek pozitif değeri doğru tahmin ettiğinin bir ölçüsüdür.\n",
    "4. Precision(Keskinlik):Pozitif olarak tahminlediğimiz değerin gerçekte kaç adedinin pozitif olduğunu gösteren bir ölçüttür.\n",
    "5. F1-Score:Duyarlılık ve keskinlik ifadelerinin bütünü ile değerlendirilmesi.Sınıflandırıcının ne kadar performans gösterdiğini gösteren bir ölçüt.\n",
    "\n",
    "2. Maliyet Fonksiyonları:Geliştirilen modelin doğru sonuca ne kadar uzak olup olmadığını ölçümlemek.Teknik tanımla geliştirilen modelden elde edilmiş bir tahmin sonucunun gerçek sonuçtan  uzaklığını matematiksel olarak ifade etme.\n",
    "\n",
    "Yapay Sinir Ağı yapısından çıkarımlar ile maliyet fonksiyonunu hesaplama;\n",
    "1. Quadratic Cost: Gerçek sonuç ile tahmin edilen sonuç arasındaki farkın karesi hesaplandığı için büyük hatalarda elde edilecek sonuç.Fakat büyük hatalar var ise elde edilmesi istenilen sonuç kat-la-na-rak artacaktır. Zaman alıcıdır.\n",
    "2. Cross Entropy: Elde edilmesi gereken gerçek sonuç ile aktivasyon fonksiyonundan geçtikten sonra elde edilen sonuç arasındaki fark ne kadar büyük ise modelin öğrenmesi o kadar hızlı olacaktır.Önerilir!\n",
    "\n",
    "MODELİN EĞİTİLMESİ:Bir yapay zeka modelinin geliştirilme süreçleri;\n",
    "\n",
    "1. Veri Toplanması: Eğitim ve test için gerekli veri setini elde etme\n",
    "2. Veri Temizlenmesi: Hatalı olan görüntüler düzeltilir ya da silinir.(Gürültü resim, farklı renk biçimi olarak kodlanılmış vb..)\n",
    "3. Veri Seti Bölünmesi: Mevcut veri setini iki parçaya bölünmesi\n",
    "4. Modelin Eğitilmesi: Veri setinin %70-80 eğitim için kullan, %20-30 ise test amaçlı kullanılabilir.\n",
    "        ^                  Geri\n",
    "        |                  Yayılım\n",
    "5. Modelin Değerlendirilmesi: Eğitim görüntü veri seti\n",
    "6. Modelin Test Edilmesi: Test görüntü veri seti uyumluluğu\n",
    "7. Modelin Kayıt Edilmesi: İstenen başarı düzeyine ulaşan model sonraki kullanımlar için kaydedilir.\n",
    "\n",
    "GRADYAN AZALTMA ve GERİ YAYILIM:\n",
    "\n",
    "Gradient Descent,bir fonksiyonun minimununu bulmak için kullanılan bir optimizasyon algoritmasıdır.Amacımız maliyet fonksiyonunu minimuma indirgemek yani gerçek sonuç ile tahmin edilen sonuç arasındaki farkı;tahmin edilen sonucun gerçeğe yaklaştırarak en aza indirmektir.Global Minimum değeri elde edilir.\n",
    "\n",
    "Backpropogation, Global minimum değeri elde edildikten sonra yapay zeka modelinde bulunan noktada geriye doğru tüm ağdaki ağırlık veya bias değeri sonucun pozitif olarak çıkmasını sağlanması amacıyla güncellenerek işlemektedir. Teknik olarak,yapılan işlev eğitim için kullanılan verisetinden bir bloğun(batch) modele uygulandıktan sonra hatalara göre;geriye doğru bütün nöronlarda ağırlık ve bias değerlerinin değiştirilmesidir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e0435-b3b1-4c8c-b48d-bba168e50578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                              MODEL OLUŞTURMA ve GÖRÜNTÜ ÜZERİNDE ÖĞRENME YAKLAŞIMLARI\n",
    "from keras.models import Sequential\n",
    "model = Sequential(*args,**kwargs) # Her bir seviyesi bir giriş bir de çıkış katmanına sahip olan düzgün olarak sıralanmış yığın katmanlar oluşturabilir.\n",
    "# args:geliştirilecek modelin ara katmanları(layers,name)(varsayılan:None) layers:modelde bulunacak ara katmanlar DENSE sınıfı & name:modele bir isim verilerek sonrasında bu isim ile erişebilme.\n",
    "\n",
    "# ! Sequential() sınıfı ile oluşturulmuş bir modele ara katmanların eklenmesinde Dense() sınıfı kullanılır.Dense() sınıfı kullandıktan sonra bir daha değiştirilemez.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(*args,**kwargs))\n",
    "\"\"\" \n",
    "args:ara katmanlar ile ilgili ayarlar;\n",
    "units:çıktı uzayının kaç boyutlu olacağı \n",
    "input_shape:giriş katmanı boyutunu ayarlamak \n",
    "input_dim:giriş katmanı tek boyutlu katmanın kaç girişli olduğu ayarlanır.\n",
    "activation:eklenen katmanda eklenecek olan aktivasyon fonksiyonunu ayarlamak(Varsayılan:None) \n",
    "use_bias:bias değerinin kullanılıp/kullanılmayacağı ayarlanır(varsayılan:True)\n",
    "bias_initializer:bias başlangıç değeri ayarlamak için kullanılır(varsayılan:zeros)\n",
    "bias_regularizer:bias değerlerinde kullanılacak düzenleyici işlevin ayarlaması\n",
    "bias_constraint:bias değerlerinde kullanılcak kısıtlama işlevin ayarlanması için kullanılır(None)\n",
    "kernel_initializer:ağırlık matrisinin başlangıç değerlerini ayarlamak için kullanılır.(varsayılan:glorot_uniform)\n",
    "kernel_regularizer:ağırlık matrislerinde kullanılcak düzenleyici işlevin ayarlaması için kullanılır(varsayılan:None)\n",
    "kernel_constraint:ağırlık matrisinde kullanılacak kısıtlama işlevin ayarlanması için kullanılır(None)\n",
    "activity_regularizer:çıkış fonksiyonunda kullanılacak düzenleyici işlevin ayarlanması için kullanılır.\n",
    "\"\"\"\n",
    "\n",
    "#16 girişli, 2 ara katmanlı ve 32 çıkışlı bir model oluşturulmaktadır.Ara katmanların birincisinin aktivasyon fonksiyonu RELU olarak ayarlanılmıştır.\n",
    "from keras.models import Sequential()\n",
    "from keras.layers import Dense()\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(16,)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(32))\n",
    "model.output_shape(None,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bbb12-31f1-4f32-8854-fd5cb3bc47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELİN DERLENMESİ\n",
    "model.compile(optimizer,loss,metrics,loss_weights,sample_weight_mode,weight_ed_metrics,target_tensors,**kwargs)\n",
    "\n",
    "# optimizer:Adam,SGD ve momentum yazılabilir.Asıl amaç salınım azaltmak için kullanılır.KERAS OPTİMİZERS keras dökümantasyonunda erişebilirim. \n",
    "Adam(learning_rate,beta_1,beta_2,epsilon,amsgrad,name,**kwargs) \n",
    "SGD(learning_rate,momentum=0.0,nesterov,name,**kwargs) \n",
    "# loss:kayıp fonksiyonların ayarlanması:binary_crossentropy:True/False & categorical_crossentropy:sonucun birden fazla sınıflar içerisinden biri.\n",
    "# metrics: eğitim ve test sırasında kullanılacak olan ölçeğin ayarlanılması\n",
    "# loss_weight: model tarafından minimize edilecek kayıp değeri\n",
    "# weighted_metrics: eğitim/test süresince sample_weight veya class_weight tarafından hesaplandırılacak ve ağırlandırılacak ölçeklerin listesi\n",
    "# run_eagerly:eğer true ile ayarlanırsa bu modelin mantığı tensorflow fonksiyonu ile çalıştırılmayacaktır.(varsayılan False)\n",
    "# steps_per_execution:modelin ek yükünü azaltmak ve performası en üst düzeye doğru artıran bir sabit sayı(varsaılan 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74d14f-8cb8-45d1-b52e-01edfe9c2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELİN EĞİTİMİ\n",
    "model.fit(x,y,batch_size,epochs,verbose,callbacks,validation_split,validation_data,shuffle,class_weight,sample_weight,initial_epoch,steps_per_epoch,\n",
    "         validation_steps,validation_freq,max_queue_size,workers,use_multiprocessing,**kwargs)\n",
    "\"\"\"\n",
    "x:Modelin eğitilmesinde kullanılacak giriş bilgileri\n",
    "y:x parametresiyle sağlanan giriş bilgilerine karşılık gelen etiketler sağlanır.\n",
    "batch_size:aynı anda işlem görecek boyut bilgisi.çok büyük olması modelin eğitilme hızını olumsuz etkiler(varsayılan:32)\n",
    "epochs:tüm veri seti üzerinde atılacak tam bir iterasyon sayısıdır.(varsayılan:1)\n",
    "verbose:model eğitilirken verilecek bilginin biçimini ayarlama, 0-1-2-auto(varsayılan:auto)\n",
    "callbacks:istenilirse belirli fonksiyonları çağırarak işlem yapılmasını sağlayan parametre\n",
    "validation_split:0 ile 1 arasında parametre olarak verilen eğitim setinden doğrulama için kullanılacak parça yüzdesel olarak belirlenir.\n",
    "validation_data:her bir epoch'ta başarı ölçütlerinin çıkarılması yani doğrulama için kullanılacak veri sağlanır\n",
    "shuffle:eğitim veri setinin her bir epoch'dan önce karıştırılıp/karıştırılmayacağını ayarlamak için kullanılır.\n",
    "class_weight:sınıf ağırlıklandırma parametresi\n",
    "sample_weight:kayıp fonksiyonları ağırlıklandırma parametresi\n",
    "initial_epoch:eğitime başlanılacak epoch numarasını vermek\n",
    "steps_per_epoch:her bir epoğun sonlandırılması için verilecek adım sayısıdır.\n",
    "validation_steps:eoğun sonlandırılmadan önce doğrulamada kullanılacak adım sayısıdır\n",
    "validation_freq:her bir doğrulama adımına geçmeden önce gerçekleştirelecek epok sayısının tanımlanması(1)\n",
    "max_queue_size:işlenen batch'lerin ön belleğe alınmasında kullanılacak sınır değeri.Paralel çalışmada akışım bozulmaması için kullandım.(10)\n",
    "workers:batches cpu'da işlendikten sorna gpu'ya gönderilir.(1)\n",
    "use_multiprocessing:işlemcide çoklu çekirdek kullanımını ayarlamak(False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8dd042-7994-45d3-9614-434490fab5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELİN EĞİTİLMEDEN ÖNCE EĞİTİM-TEST VERİSETLERİNE BÖLÜNMESİ\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "tts(*arrays,test_size,train_size,random_state,shuffle,stratify)\n",
    "\"\"\"\n",
    "arrays:aynı uzunlukta,boyutta ve biçimde bölünecek dizi\n",
    "test_size:float(0.0-1.0 arasında),int(test verisi kullanılcak adet),None(train_size parametresi ile kendsini 1.0'a tamamlayacak sayı değeri)\n",
    "train_size:float(0.0-1.0 arasında),int(test verisi kullanılcak adet),None(train_size parametresi ile kendsini 1.0'a tamamlayacak sayı değeri)\n",
    "random_state:ayırma işlemi yapılmadan önce veri setinin rastgele karıştırılması.Değer verilirse her seferinde aynı karışımı yapılabilir.\n",
    "shuffle:verinin ayrılmadan önce karıştırılıp/karıştırılmayacağını ayarlamak için kullanılır.\n",
    "stratify:veri setinin bölünmesinde kullanılacak bir dizi bilgisi sağlanabilir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d759cb-9f33-4155-af90-f226e9ad94d0",
   "metadata": {},
   "source": [
    "**Eğer çalışılan verilerin sayı aralıkları birbirinden çok bağımsız ise model eğitimi esnasında hatalı sonuçların çıkmasına veya aşırı öğrenmiş bir modelin oluşmasına sebep olabilir. Bu tür durumlarda veri setinde bulunan sayısal rakamlar normalize edilerek kullanılmaktadır.Bir veri sütunun normalize edilmesi, bu veri sütununda bulunana bütün elemanların belirli bir başlangıç-bitiş aralığı içerisinde dağıtılmasına veya standart sapma, ortalamaları gibi değişkenlerin baz alarak belirli bir ölçek içerisinde dağıtılmasını sağlamak anlamına gelmektedir.**\n",
    "\n",
    "Normalize: MinMaxScaler() sınıfı,veri sütunundaki dağılım yine bu fonksiyona sağlanan parametreler ile ayarlanabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f339c2-bb2f-4985-bd56-5b2db9748683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaled = MinMaxScaler(feature_range,*,copy,clip)\n",
    "\"\"\" \n",
    "feature_range:tuple veri türünde normalize edilecek aralığın ilk ve son değerleri verilir.(varsayılan: (0,1))\n",
    "copy:eğer giriş zaten bir numpy dizisi ise kopyasında kaçmak için False,varsayılan:True\n",
    "clip:eğer true ise ölçeklenen veriler feature_range parametresi ile sağlanan aralığa kırpılacaktır.varsayılan:False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11052a8-8f3b-440f-921f-61d330f2bd51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71 (284.00 Byte)\n",
      "Trainable params: 71 (284.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 - 1s - loss: 0.6608 - accuracy: 0.5578 - 656ms/epoch - 20ms/step\n",
      "Epoch 2/50\n",
      "33/33 - 0s - loss: 0.6442 - accuracy: 0.5578 - 55ms/epoch - 2ms/step\n",
      "Epoch 3/50\n",
      "33/33 - 0s - loss: 0.6343 - accuracy: 0.5578 - 42ms/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "33/33 - 0s - loss: 0.6241 - accuracy: 0.5714 - 48ms/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "33/33 - 0s - loss: 0.6134 - accuracy: 0.6278 - 54ms/epoch - 2ms/step\n",
      "Epoch 6/50\n",
      "33/33 - 0s - loss: 0.6019 - accuracy: 0.6803 - 47ms/epoch - 1ms/step\n",
      "Epoch 7/50\n",
      "33/33 - 0s - loss: 0.5898 - accuracy: 0.6900 - 74ms/epoch - 2ms/step\n",
      "Epoch 8/50\n",
      "33/33 - 0s - loss: 0.5757 - accuracy: 0.6948 - 56ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "33/33 - 0s - loss: 0.5607 - accuracy: 0.7075 - 46ms/epoch - 1ms/step\n",
      "Epoch 10/50\n",
      "33/33 - 0s - loss: 0.5451 - accuracy: 0.7532 - 43ms/epoch - 1ms/step\n",
      "Epoch 11/50\n",
      "33/33 - 0s - loss: 0.5299 - accuracy: 0.7629 - 48ms/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "33/33 - 0s - loss: 0.5146 - accuracy: 0.7891 - 39ms/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "33/33 - 0s - loss: 0.4989 - accuracy: 0.7979 - 32ms/epoch - 973us/step\n",
      "Epoch 14/50\n",
      "33/33 - 0s - loss: 0.4829 - accuracy: 0.8047 - 36ms/epoch - 1ms/step\n",
      "Epoch 15/50\n",
      "33/33 - 0s - loss: 0.4659 - accuracy: 0.8202 - 36ms/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "33/33 - 0s - loss: 0.4485 - accuracy: 0.8251 - 37ms/epoch - 1ms/step\n",
      "Epoch 17/50\n",
      "33/33 - 0s - loss: 0.4312 - accuracy: 0.8367 - 49ms/epoch - 1ms/step\n",
      "Epoch 18/50\n",
      "33/33 - 0s - loss: 0.4143 - accuracy: 0.8465 - 38ms/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "33/33 - 0s - loss: 0.3962 - accuracy: 0.8591 - 29ms/epoch - 868us/step\n",
      "Epoch 20/50\n",
      "33/33 - 0s - loss: 0.3783 - accuracy: 0.8630 - 48ms/epoch - 1ms/step\n",
      "Epoch 21/50\n",
      "33/33 - 0s - loss: 0.3611 - accuracy: 0.8669 - 35ms/epoch - 1ms/step\n",
      "Epoch 22/50\n",
      "33/33 - 0s - loss: 0.3434 - accuracy: 0.8766 - 44ms/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "33/33 - 0s - loss: 0.3264 - accuracy: 0.8805 - 45ms/epoch - 1ms/step\n",
      "Epoch 24/50\n",
      "33/33 - 0s - loss: 0.3108 - accuracy: 0.8950 - 38ms/epoch - 1ms/step\n",
      "Epoch 25/50\n",
      "33/33 - 0s - loss: 0.2955 - accuracy: 0.8980 - 50ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "33/33 - 0s - loss: 0.2782 - accuracy: 0.9057 - 50ms/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "33/33 - 0s - loss: 0.2631 - accuracy: 0.9174 - 33ms/epoch - 1000us/step\n",
      "Epoch 28/50\n",
      "33/33 - 0s - loss: 0.2476 - accuracy: 0.9213 - 59ms/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "33/33 - 0s - loss: 0.2330 - accuracy: 0.9300 - 40ms/epoch - 1ms/step\n",
      "Epoch 30/50\n",
      "33/33 - 0s - loss: 0.2182 - accuracy: 0.9320 - 41ms/epoch - 1ms/step\n",
      "Epoch 31/50\n",
      "33/33 - 0s - loss: 0.2060 - accuracy: 0.9388 - 37ms/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "33/33 - 0s - loss: 0.1917 - accuracy: 0.9417 - 50ms/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "33/33 - 0s - loss: 0.1784 - accuracy: 0.9436 - 50ms/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "33/33 - 0s - loss: 0.1662 - accuracy: 0.9466 - 33ms/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "33/33 - 0s - loss: 0.1574 - accuracy: 0.9553 - 50ms/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "33/33 - 0s - loss: 0.1438 - accuracy: 0.9553 - 39ms/epoch - 1ms/step\n",
      "Epoch 37/50\n",
      "33/33 - 0s - loss: 0.1328 - accuracy: 0.9602 - 35ms/epoch - 1ms/step\n",
      "Epoch 38/50\n",
      "33/33 - 0s - loss: 0.1210 - accuracy: 0.9679 - 35ms/epoch - 1ms/step\n",
      "Epoch 39/50\n",
      "33/33 - 0s - loss: 0.1117 - accuracy: 0.9689 - 37ms/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "33/33 - 0s - loss: 0.1006 - accuracy: 0.9718 - 31ms/epoch - 953us/step\n",
      "Epoch 41/50\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9747 - 33ms/epoch - 999us/step\n",
      "Epoch 42/50\n",
      "33/33 - 0s - loss: 0.0871 - accuracy: 0.9767 - 50ms/epoch - 2ms/step\n",
      "Epoch 43/50\n",
      "33/33 - 0s - loss: 0.0806 - accuracy: 0.9776 - 34ms/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "33/33 - 0s - loss: 0.0756 - accuracy: 0.9786 - 37ms/epoch - 1ms/step\n",
      "Epoch 45/50\n",
      "33/33 - 0s - loss: 0.0715 - accuracy: 0.9786 - 50ms/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "33/33 - 0s - loss: 0.0677 - accuracy: 0.9786 - 42ms/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "33/33 - 0s - loss: 0.0636 - accuracy: 0.9815 - 46ms/epoch - 1ms/step\n",
      "Epoch 48/50\n",
      "33/33 - 0s - loss: 0.0599 - accuracy: 0.9815 - 40ms/epoch - 1ms/step\n",
      "Epoch 49/50\n",
      "33/33 - 0s - loss: 0.0571 - accuracy: 0.9854 - 50ms/epoch - 2ms/step\n",
      "Epoch 50/50\n",
      "33/33 - 0s - loss: 0.0545 - accuracy: 0.9845 - 48ms/epoch - 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "[[185   3]\n",
      " [  0 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       188\n",
      "         1.0       0.98      1.00      0.99       155\n",
      "\n",
      "    accuracy                           0.99       343\n",
      "   macro avg       0.99      0.99      0.99       343\n",
      "weighted avg       0.99      0.99      0.99       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "data = np.genfromtxt('OneDrive/Belgeler/OpenCV/Kitap/DATA/bank_note_data.txt',delimiter=',')\n",
    "y = data[:,4]\n",
    "X = data[:,0:4]\n",
    "\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.25)\n",
    "\n",
    "scaled = MinMaxScaler()\n",
    "scaled.fit(X_train)\n",
    "scaled_X_train = scaled.transform(X_train)\n",
    "scaled_X_test = scaled.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(4,input_dim=4,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(scaled_X_train,y_train,epochs=50,verbose=2)\n",
    "\n",
    "predictions = (model.predict(scaled_X_test) > 0.5).astype(\"int32\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee70b9e-3b9e-41d9-a09e-b1370772a58d",
   "metadata": {},
   "source": [
    "**fit() metodunun sadece eğitim verisiyle kullanıldığına dikkat edilmelidir.Yani ele alınan veriler bir defa bu metoda sağlanan veri seti ile normalizasyon için gerekli olan değerlerin hesaplanması gerçekleştirilmektedir.Sonrasında ise bu değerler kullanılarak normalizasyon yapılmaktadır.Yani test aşamasına geçildiğinde tekrar fit() metodunun kullanılmaması gerekmektedir.Çünkü modelin eğitiminde hangi normalizasyon ayarlamaları kullanıldıysa sonrasında yeni gelecek değerler bu ayarlamalar kullanılarak normalize edilecek ve kullanılacaktır.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4479f-d268-43ce-aad3-451d39255e1f",
   "metadata": {},
   "source": [
    "**2-)EVRİŞİMLİ SİNİR AĞLARI(CONVOLUTİONAL NEURAL NETWORK)**\n",
    "\n",
    "Biyolojik görsel korteksin çalışmasında araştırmacılar bazı görevlerde sadece belirli bir kısım nöronun etkileşime girdiğini tespit etmişlerdir.Bu kortekslerin çalışmasından ilham alınarak geliştirilen CNN modeli de benzer şekilde normal bir derin öğrenme modelinde olduğu gibi her katmandaki nöronun diğer katmandaki nöronlarla bağlantısı olması gerekliliği yoktur. Yani bazı nöronlar bir sonraki katmanda sadece belirli bir kısım nöronla etkileşime girecektir.\n",
    "\n",
    "CNN modelinin görüntüler üzerinde nasıl çalışıldığına bakılacak ise;\n",
    "1. Görüntü sayısal değerlerle dolu bir matris ile temsil edilebilen bir değişkenden başka bir şey değildir.Bu özelliğin öğrenilmesi aslında matrisin öğrenilmesi ve sonraki karşılaşmalarda kullanılmasından ibarettir.Modele sağlanan sayılardan öğrenerek tahmin geliştirme gibi düşünülebilir.\n",
    "2. Eğer kullanılan resim renkli resim ise bu matristen her bir renk kanalı için birer tane olacağından dolayı 3 adet matris kullanılarak öğrenme sağlanılacaktır.Çözünürlüğün veya piksel değerinin büyük olduğu durumda veya canlı video akışlarında süreci Evrişimli Katman(Convolution Layer) ve Havuzlama Katmanı(Pooling Layer) devreye girecektir.\n",
    "\n",
    "**EVRİŞİMLİ KATMAN(CONVOLUTİON LAYER)**\n",
    "\n",
    "Evrişimli katman bir grup pikselin daha basit olarak ifade edilmesidir.Bunu sağlamak için özellikleri çıkarılacak olan görüntünün piksel değerleri belirlenen bir kernel yapısıyla işleme tabi tutularak küçültme işlemi yani evrişim sağlanılır bu sayede Evrişimli Katman elde edilmiş olur.Hesaplamalar bölüm 3'de anlatılan **filter2d()** gibidir.\n",
    "\n",
    "Kernel matrisinde olan her bir eleman sırasıyla görüntü üzerinde karşılığında olduğu değer ile çarpılarak toplanıp yeni bir matris elde edilmektedir.Sonuç olarak elde edilen Evrişim katmanının orijinalden daha küçük olduğu görülmektedir.Bunun sebebi kernel görüntüsünün kenarlarına oturtulmamasından kaynaklanmaktadır. Bu yüzden kullanılan kernel biçimi ile bağlantılı olarak giriş görüntüsünde küçülme olacaktır.Bu işleme Yerinde Dolgu **Valid Padding** denilir.Aslında bu işlem iki türlü yapılabilir;\n",
    "\n",
    "1. Eğer orijinal görüntü resmi dışarısına bir kenarlık eklendikten sonra kernel ile çarpma işlemi gerçekleşirse sonuç olarak elde edilen görüntünün boyutu giriş görüntüsü ile aynı kalacaktır.**Aynı Dolgu(Same Padding)**\n",
    "2. Eğer giriş resmi renkli ise bu işlem her renk kanalında ayrı ayrı yapılarak sonuç üretilecektir.\n",
    "\n",
    "**HAVUZLAMA KATMANI(POOLİNG LAYER)**\n",
    "\n",
    "Görüntünün küçültülmesi veya daha teknik olarak giriş resminin daha küçük bir boyutlu görüntüler ile ifade edilmesi için Pooling Katmanı devreye girecektir.Havuzlama katmanı büyük görüntülerin baskın yanını ayırt edici özelliklerinin daha küçük boyutlarda saklanmasını sağlayan katmanlardır.\n",
    "\n",
    "Maksimum Havuzlama ve Ortalama Havuzlama çeşitleri mevcuttur.Biri çalıştığı tarama alanında en büyük değerleri kullanırken,diğeri çalıştığı tarama alanında bulunan değerlerin ortalamasını alır.Özellikle gürültülü resimlerde Maksimum Havuzlama daha çok tercih edilir.\n",
    "\n",
    "**GENİŞLETİLMİŞ EVRİŞİMLER(DİLATED CONVOLUTİONS)**\n",
    "\n",
    "Küçük bir kernel yapısının ek performans ve maliyet gerektirmeden daha büyük bir alana etki etmesi için kullanılır.Burada verilen değer ile kernel yapısında değerler arasında boş satır ve sütunlar eklenir.Bu yöntem kullanılarak aynı hesaplama maliyeti ile daha fazla alanın incelenmesi sağlanılabilir. \n",
    "\n",
    "**TAM BAĞLANTILI KATMAN(FULLY CONNECTED LAYER)**\n",
    "\n",
    "Bir evrişim sinir ağındaki son katman sınıflandırma sonucunun üretilmesi için gerekli çıktının sağlandığı katmandır.Bu katmanın amacı tahmin sonunun üretilmesi ve elde edilen sonucun değerlendirilmesi için klasik bir sinir ağı katmanına(Çıkış Katmanına) yönlendirilmesidir.\n",
    "\n",
    "Daha teknik olarak yaptığı işlev Evrişim ve Havuzlama katmanlarından elde edilen sonuçlar ile sınıflama etiketlerinin üretileceği klasik bir sinir ağı arasında iletişim kurmaktır. Çünkü yapısal olarak çıkış katmanı ile CNN katmanları arasında elde edilen sonuçlar birbirlerine uygun değildir.Bu uygunluğun sağlanması için evrişim ve havuzlama katmanlarında elde edilen sonuçlar düzleştirerek tek boyut haline getirilmektedir.**Düzleştirme Katmanı.** \n",
    "\n",
    "Düzleştirilmiş katmandan elde edilen sonuç amaca uygun SoftMax veya Sigmoid gibi bir fonksiyondan geçirildikten sonra giriş resmine uygun olan etiket üretilir.\n",
    "\n",
    "\n",
    "**Makina Öğrenme Algoritmaları doğrudan kategorik veriler üzerinde çalışmadığı için bu kategorilerin sayısal olarak ifade edilmeleri gerekmektedir.Bu kategorik çıkışların sayısal olarak ifade edilmesine **Label Encoding** ve **One Hot Encoding** kullanımı yardımımıza yetişecektir.\n",
    "\n",
    "Label Encoding yönteminde her bir veri çeşidi kategorisi için alfabetik sıralamaya göre benzersiz bir tam sayı atanır.One Hot Encoding kategorik değişkenlerin ikilik sayı sisteminden oluşan vektörel bir yapıyla ifade edilmesinde kullanılan bir yöntemdir.\n",
    "\n",
    "OpenCV'de kategorik bir veri setinin ikili sayı sisteminde sayısallaştırılmasında **keras'tan to_categorical()** metodu bulunabilir.\n",
    "\n",
    "\n",
    "**DÜZLEŞTİRME KATMANI(FLATTEN LAYER)**\n",
    "\n",
    "**Flatten()** Bu adımın amacı elde edilen çok boyutlu çıktıların klasik bir Dense ağı ile bağlayarak sonuç üretmektedir.\n",
    "\n",
    "Evrişim ağından elde edilen sonuçlar kullanılan eğitim seti görüntü ise (genişlik,yükseklik,kanal) şeklinde, kullanılan eğitim seti video ise(çerçeve,genişlik,yükseklik,kanal) şeklinde.Ancak bu sonuçların klasik bir Dense yapısıyla birleştirilmesi mümkün değildir.Çünkü Dense yapıları çok boyutluluğu desteklemektedir.Bu sebeple kullanılacak Flatten() metodu ile çok boyutlu olarak elde edilen sonuçlar tek boyuta çevrilerek **DENSE** yapısına uygun hale getirilmektedir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e244d9-48d1-4311-8a22-6034f0783da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical as tc\n",
    "y_test-train_cat = tc(y,num_classes,dtype)\n",
    "\"\"\"\n",
    "y:ikili sayı türü ile ifade edilecek kategorik sınıf bilgisi sağlanır.\n",
    "num_classes:toplamda kaç sınıf olacağı bilgisi sağlanılır.\n",
    "dtype:çıkış matrisinin veri türünü ayarlamak için yapılır.\n",
    "\"\"\"\n",
    "\n",
    "# GÖRÜNTÜNÜN ÖZELLİKLERİ(genişlik,yükseklik,kanal) şeklinde olduğu için Conv2D() metodunun kullanılması uygundur.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters,kernel_size,strides,padding,data_format,dilation_rate,activation,use_bias,kernel_initializer,bias_initializer,\n",
    "                 kernel_regularizer,bias_regularizer,activity_regularizer,kernel_constraint,bias_constraint,**kwargs))\n",
    "\"\"\"\n",
    "filters:özelliklerin çıkarımda kullanılacağı filtre sayısıdır.\n",
    "kernel_size:2 boyutlu evrişim ağının genişlik ve yükseklik parametrelerini ayarlamak\n",
    "strides:yatay,dikey kayma oranı\n",
    "padding:filtrenin sol en üstteki değeri ile görüntünün sol en üstteki değeri üst üste gelecek şekilde hesaplamak\n",
    "data_format:{batch,channels,height,weight}\n",
    "dilation_rate:kullanılan kernel değerleri arasındaki boşluğun artılarak küçük bir kernel yapısı ile görünütüye büyük bir açıyla bakılması gibi düşünülebilir.\n",
    "activation:string olarak kullanılacak aktivasyon fonksiyon değeri saklanır.\n",
    "use_bias:evrişim katmanına düzeltmeler için bir bias değerinin eklenip eklenmeyeceğini ayarlamak için kullanılır\n",
    "kernel_initializer:kernel değerlerin başlangıcını ayarlamajk için kullanılır\n",
    "......\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPool2D\n",
    "model = Sequential()\n",
    "model.add(MaxPool2D((pool_size,strides,padding,data_format,**kwargs))\n",
    "          \n",
    "\"\"\"\n",
    "pool_size:dikey-yatay kullanılacak ölçeği verilir(varsayılan:(2,2))\n",
    "strides:kullanılan yapının kayma miktarını ayarlamak için yapılır.(varsayılan:'valid')\n",
    "padding:kullanılan yapının kenar giriş matrisinde kenarları kullanıp/kullanılmamasını ayarlamak için(varsayılan:'valid')\n",
    "data_format:{batch,channels,height,weight}(varsayılan:None)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "model = Sequential()\n",
    "model.add(Flatten(data_format=None,**kwargs))\n",
    "        \n",
    "#data_format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58ef6623-5b7a-4f44-90f1-b9a36e22d260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 25, 25, 32)        544       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 12, 12, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 591786 (2.26 MB)\n",
      "Trainable params: 591786 (2.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.7608 - accuracy: 0.9467\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0708 - accuracy: 0.9814\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 968,    0,    5,    0,    0,    1,    2,    1,    2,    1],\n",
       "       [   0, 1129,    2,    2,    0,    2,    0,    0,    0,    0],\n",
       "       [   1,    3, 1009,    2,    0,    0,    1,    6,    9,    1],\n",
       "       [   0,    0,    6,  967,    0,   25,    0,    2,    7,    3],\n",
       "       [   1,    0,    1,    0,  963,    0,    5,    2,    0,   10],\n",
       "       [   1,    0,    0,    0,    0,  886,    1,    0,    1,    3],\n",
       "       [   5,    2,    1,    0,    3,    7,  936,    0,    3,    1],\n",
       "       [   0,    3,   14,    3,    4,    1,    0,  995,    2,    6],\n",
       "       [   3,    1,    2,    1,    1,    6,    0,    2,  953,    5],\n",
       "       [   1,    2,    0,    3,    8,    8,    0,    4,    2,  981]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MNIST veri seti ile el yazısı rakamlarını tanımlama\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test  = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(28,28,1),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train,y_train_cat,epochs=2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "confusion_matrix(y_test, y_pred_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce1c31f-73bb-4302-99f5-334025e48e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 29, 29, 32)        1568      \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 14, 14, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 11, 11, 32)        16416     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 5, 5, 32)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 800)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               205056    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225610 (881.29 KB)\n",
      "Trainable params: 225610 (881.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.5099 - accuracy: 0.1070\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3383 - accuracy: 0.1033\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 2.3256 - accuracy: 0.1024\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 2.3199 - accuracy: 0.1010\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 2.3094 - accuracy: 0.1021\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 2.3198 - accuracy: 0.1009\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 37s 23ms/step - loss: 2.3384 - accuracy: 0.1004\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 2.3034 - accuracy: 0.1004\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.3064 - accuracy: 0.0983\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3090 - accuracy: 0.0996\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.11      0.11      1000\n",
      "           1       0.08      0.09      0.08      1000\n",
      "           2       0.10      0.10      0.10      1000\n",
      "           3       0.10      0.09      0.09      1000\n",
      "           4       0.09      0.09      0.09      1000\n",
      "           5       0.12      0.11      0.11      1000\n",
      "           6       0.10      0.10      0.10      1000\n",
      "           7       0.11      0.11      0.11      1000\n",
      "           8       0.10      0.10      0.10      1000\n",
      "           9       0.10      0.10      0.10      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.10      0.10      0.10     10000\n",
      "weighted avg       0.10      0.10      0.10     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CIFAR-10 ile GÖRÜNTÜ TANIMA\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = cifar10.load_data()\n",
    "\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train,y_train_cat,verbose=1,epochs=10)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "confusion_matrix(y_test,y_pred_classes)\n",
    "\n",
    "print(classification_report(y_test,y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a1f83-c72e-49b9-a5ac-2b1f99f6cbbf",
   "metadata": {},
   "source": [
    "Bir resmin farklı açılarda ve büyüklüklerdeki durumları kullanılarak,eğitim aşamasında bu resimlerden elde edilebilecek özelliklerin çoğaltılması mümkündür.\n",
    "\n",
    "OPENCV'de .jpeg , .png , .bmp , .gif uzantılarını destekleyen **ImageDataGenerator()** sınıfı bulunmaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25108280-5d9b-4409-944a-e2298ad42a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageGen = ImageDataGenerator(featurewise_center,samplewise_center,featurewise_std_normalization,samplewise_std_normalization,zca_whitening,zca_epsilon,\n",
    "                              rotation_range,width_shift_range,height_shift_range,brightness_range,shear_range,zoom_range,channel_shift_range,fill_mode,\n",
    "                             cval,horizontal_flip,vertical_flip,rescale,preprocessing_function,data_format,validation_split,interpolation_order,dtype)\n",
    "\n",
    "\"\"\"\n",
    "featurewise_center:eger True olarak ayarlanırsa ortalama piksel değeri 0 olacak şekilde ölçeklendirilecektir.\n",
    "samplewise_center:eger True olarak ayarlanırsa her bir resim için ayrı ayrı ortalama piksel değeri 0 olacak şekilde ölçeklendirilecektir.\n",
    "featurewise_std_normalization:eger True olarak ayarlanırsa her bir resim bütün veri setinin standart sapması ile bölünecektir.\n",
    "samplewise_std_normalization:eger True olarak ayarlanırsa her bir resim ayrı ayrı kendi standart sapması ile bölünecektir.\n",
    "zca_whitening:görüntülerin beyazlaştırılması ve veri yapısını bozmadan gereksiz fazlalıkların kaldırılması için kullanılır.\n",
    "zca_epsilon:beyazlaştırma işleminde kullanılacak epsilon değeridir(1e-06)\n",
    "rotation_range:görüntülerin rastgele döndürerek kullanılacağı açının değeridir.\n",
    "width_shift_range:görüntünün yatayda hareket edilmiş şekli ile görüntününü kendi aralığında hareket etmek için kullanılır.\n",
    "height_shift_range:görüntünün dikeyde hareket edilmiş şekli ile görüntünün kendi aralığında hareket etmek için kullanılır.\n",
    "brightness_range:görüntülerin parlak ve koyu durumları ile yeni görüntüler üretmek için kullanılır.\n",
    "shear_range:görüntünün bir kenarı X,Y eksenleri boyunca yamultularak orijinal görüntünün yeni kopyalarını oluşturur.\n",
    "zoom_range:giriş görüntüsünün yakınlaşmış versiyonları ile yeni görüntüler oluşturulmak için kullanılır.\n",
    "channel_shift_range:giriş görüntülerinin kanal değerlerinin RGB değerlerinde verilen değer kadar kaydırma yapılarak yeni görüntüler üretilmesini sağlar.\n",
    "fill_mode:resim kaydırma işleminden sonra açılan boşluğun nasıl doldurulacağını ayarlamak için 'nearest'\n",
    "horizontal_flip:giriş görüntüsünü rastgele olarak dikey yönde çevirmek için kullanılır.\n",
    "vertical_flip:giriş görüntüsünü rastgele olarak yatay yönde çevirmek için kullanılır.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732272e1-df81-447d-a12f-26a49a4587cd",
   "metadata": {},
   "source": [
    "ImageDataGenerator() sınıfından elde edilen nesne yeni üretilecek resimlerin bütün özellikleri içermektedir.Türetilecek resimlerin orjinal hallerinin bulunduğu adres yolu ve diğer ayarlamalar nesne üzerinde çalıştırılabilecek **flow_from_directory()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2710af-8c30-4ebf-8144-85cec8e59e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traind_or_test = imageGen.flow_from_directory(directory,target_size,color_mode,classes,class_mode,batch_size,shuffle,seed,\n",
    "                                                save_to_dir,save_prefix,save_format,follow_links,subset,interpolation)\n",
    "\n",
    "\"\"\"\n",
    "directory:string olarak adres yolu verilir.resim formatları .png,.jpg,.bmp,.ppm,.tıf şeklindedir.\n",
    "target_size:bütün resimlerin boyutlarını(height-weight) şeklinde tekrar ayarlamak için kullanılır.\n",
    "color_mode:resimlerin çevrileceği renk kodlamasını ayarlamak için kullanılır.grayscale-rgb-rgba\n",
    "classes:eğitimde kullanılacak sınıf etiketlerinin ayarlanması için kullanılır.\n",
    "class_mode:->categorical:2D one-hot encoded -> binary:1D ikili sayı etiketi -> sparse:1D integer sayı etiketi -> input:otokodlayıcı -> None:etiket yok\n",
    "batch_size:tüm resimler burada belirlenen parça sayı kadar parçalara ayrılarak işlenir.(varsayılan:32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f402efcd-5e6f-46ac-aa3b-0b5730bf26dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 1 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 36, 36, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 34, 34, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 17, 17, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9248)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1183872   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1203393 (4.59 MB)\n",
      "Trainable params: 1203393 (4.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 54s 352ms/step - loss: 0.0059 - accuracy: 0.9950 - val_loss: 1.2740e-26 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 47s 313ms/step - loss: 2.4618e-22 - accuracy: 1.0000 - val_loss: 1.6987e-20 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 46s 306ms/step - loss: 5.4518e-17 - accuracy: 1.0000 - val_loss: 1.7061e-23 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 46s 306ms/step - loss: 2.3011e-16 - accuracy: 1.0000 - val_loss: 1.4440e-20 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 45s 298ms/step - loss: 5.0493e-16 - accuracy: 1.0000 - val_loss: 5.8890e-21 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 44s 295ms/step - loss: 2.4451e-18 - accuracy: 1.0000 - val_loss: 3.7524e-22 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 44s 293ms/step - loss: 8.7157e-20 - accuracy: 1.0000 - val_loss: 1.0195e-27 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 43s 288ms/step - loss: 1.0316e-17 - accuracy: 1.0000 - val_loss: 2.0223e-22 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 43s 288ms/step - loss: 1.0525e-17 - accuracy: 1.0000 - val_loss: 2.8183e-24 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 42s 279ms/step - loss: 3.5678e-18 - accuracy: 1.0000 - val_loss: 7.7241e-25 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9950000047683716, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN ile kedi-köpek görüntüsünün ayırım modeli\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dropout,Flatten,Conv2D,MaxPooling2D,Dense\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imageGen = ImageDataGenerator(rotation_range=30,width_shift_range=0.1,height_shift_range=0.1,rescale=1/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,vertical_flip=False,fill_mode='nearest')\n",
    "\n",
    "X_train = imageGen.flow_from_directory('OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/train',target_size=(150,150),batch_size=16,class_mode='binary')\n",
    "X_test = imageGen.flow_from_directory('OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/test',target_size=(150,150),batch_size=16,class_mode='binary')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(150,150,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(150,150,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(150,150,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "results = model.fit_generator(X_train,epochs=10,steps_per_epoch=150,validation_data=X_test,validation_steps=12)\n",
    "results.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db12a40-8e70-4f4d-9cc5-ed5b8c685b43",
   "metadata": {},
   "source": [
    "**YOLO(YOU ONLY LOOK ONCE)**\n",
    "\n",
    "Klasik nesne tespit,takip ve tanıma algoritmaları önce nesnelerin olduğu konumları tespit etmekte ve sonraki adımda ise tanımlama işlemlerini gerçekleştirdikleri için gerçek zamanlı akışlarda kullanımları oldukça problem olmaktaydı.Ancak YOLO çalışma prensibi olarak nesne takip ve tanıma işlemini aynı adımda gerçekleştirilmektedir.Diğerlerinden hızlıdır.\n",
    "\n",
    "Bilgisayarlı Görme;tespit,takip,tanımlama çok zor!\n",
    "\n",
    "1. Tespit: Görüntüde bir nesne olup olmadığının tespiti,benzer şekilde eğer nesne varsa kaç adet nesne olduğu sayısının tespiti oldukça zorlayıcıdır.Bir de buna görüntü üzerinde birkaç nesnenin üst üste gelme olasılığı süreci oldukça değişecektir.\n",
    "\n",
    "2. Takip: Nesne varlığını tespit etmekte yaşanılan zorluğun bir video akışında veya canlı bir videoda her bir çerçevede yapılması gerekliliği bu adımında karşılaşılan zorluluğun FPS oranı kadar katlanması anlamına gelmektedir.\n",
    "\n",
    "3. Tanımlama: Milyonlarca nesne örneği olduğunu düşünürsek bir görüntü çerçevesinde bulunan nesnenin ne olduğunun doğru bir şekilde tanımlanması için ne kadar karşılaştırma yapılacağı ucu açık bir kavram olacaktır.Ayrıca buna ek olarak aynı nesnenin farklı ışık açılarıyla çekilmiş biçimleri ve farklı boyutları eklendiğinde zorluğun derecesi gittikçe artacaktır.\n",
    "\n",
    "YOLO görüntü içerisinde nesne taraması yapmaz yani nesne aramaz bunun yerine görüntüyü belirli sayıda ızgaralara bölüp nesne tanımlama işlemi zorunluluğunu bölünmüş bu ızgaralara verir. Algoritmanın tek bir sürecinde hem tarama hem de tanımlama işlemi yapılmaktadır;YOLO'nun hızlı olmasının altında yatan temel sebeplerden birisi,klasik algoritmalar gibi çıkarılan bir özelliğin sonraki adımlarda taramada kullanılması yerine,algoritmanın aynı çalışma adımında hem nesne konumunu hem de nesne sınıfının tespitini yapması ve regresyon tabanlı olmasıdır.\n",
    "\n",
    "Ağ yapısını kullanan YOLO'nun çalışma sürecini parçalara bölerek inceleyelim:\n",
    "\n",
    "1. Giriş olarak alınan görüntünün S X S boyutunda eşit ızgaralara bölünmesidir.Her bir ızgara kendi içerisinde bir nesne olup/olmadığının ve ayrıca bu nesnenin orta noktasının kendi içerisinde olup olmadığının tespitinden sorumludur.Eğer ilgili bir ızgara bir nesne görüntüsü orta noktasının kendi içerisinde olduğunu algılarsa benzer şekilde ilgili nesnenin genişlik,boy ve sınıf gibi bilgilerinin tespit edilmesinden sorumludur.\n",
    "\n",
    "2. Her bir ızgarada güven değerinin hesaplanmasıdır. Bu güven değerinin ızgaranın içerdiği nesne içeriğinden ne kadar emin olduğunun matematiksel olarak ifadesidir. Eğer hücre içerisinde herhangi bir nesne bulunmuyorsa güven değeri sıfır(0) olarak hesaplanacaktır.Eğer ızgara herhangi bir nesne parçasını içeriyorsa o zaman değeri IoU = ortak_kesim_alanı / tum_alan olacaktır. IoU değeri tahmin edilen ve gerçek olan alanlar arasındaki orandır. Güven değeri 0 ile 1 arasında bir nevi olasılık değeri gibi de düşünülebilir.Eğer bu değer 0 ise ilgili ızgara içerisinde herhangi bir nesne bulunmadığı, eğer değer 1 ise ilgili ızgara içerisinde bir nesne parçasının olduğu anlamı çıkarılmaktadır.\n",
    "\n",
    "3. Her bir ızgaranın içerdiği nesne bölümü ile ilgili tahmin değerlerinin üretilmesidir.Bu tahmin değerleri ve vektörel biçime sahip bir çıkış dizisidir. **y = [pc,bx,by,bh,bw,c1,c2,..,cn]** \n",
    "pc:güven değeri - bx&by:tespit edilen nesnenin orta noktası - bh&bw:tespit edilen nesnenin boyu&genişliği - cn:hangi sınıfa ait değeri\n",
    "\n",
    "4. Maksimum Olmayan Bastırma tekniği ile fazlalık olan kutuların elenmesidir.Çünkü algoritmanın nesne tanımlama sürecinde bir nesne için bir çok kutucuk elde edilecektir.Bu kutuların elenmesi için ızgara parçanın çıkış vektöründe hesaplanan tahmin değerleri ile kullanılarak eleme işlemi gerçekleştirilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdcef6a-473a-4316-b66e-aebbc6a97c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cpu\n",
      "torchvision version:  0.8.2+cu110\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"PyTorch version:\",torch.__version__)\n",
    "print(\"torchvision version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9a1fc3-ed81-42ef-b415-b3d51d914c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged into Roboflow. To make a different login, run roboflow.login(force=True).\n"
     ]
    }
   ],
   "source": [
    "import roboflow\n",
    "roboflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d4547-67e6-4c07-9b65-04e2a6c66505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow()\n",
    "workspace = rf.workspace(\"WORKSPACE_URL\")\n",
    "project = workspace.project(\"PROJECT_URL\")\n",
    "version = project.version(\"VERSION_NUMBER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d1073-2024-4597-a06d-bb7fe97858a2",
   "metadata": {},
   "source": [
    "**YOLO algoritmasının modellenme süreçleri;**\n",
    "\n",
    "1. Veri setinin elde edilmesi:YOLO eğitim veya test için etiketlenmiş verilerin ayrı ayrı uygun yapıda klasörler içinde olmasını bekler.**ROBOFLOW**.Burada görüntünün etiketlenmesinden ziyade bu görüntüde sınıflandırılması yapılacak nesnenin bulunduğu konumun tanımlanmasının çıkarılması.Bu tanımlamanın üretilmesinde JSON-XML-CSV-TXT desteklenen türlerden birisi kullanılmalı.txt seçilirse dosyaya ek olarak .yaml dosyası oluşturulur.{class x_center y_center width height}\n",
    "\n",
    "2. YOLO gerekliliklerinin kurulması:Gereklilik olan paketlerin bilgileri indirilecek olan YOLO kodlarının içerisinde bulunan **requirements.txt** dosyasında yer almaktadır. **Kullanılacak YOLO versiyonuna göre bütün paketlerin versiyonlarının istenilen versiyon numarasına sahip olması gerekliliğidir.Yani ilgili paketin güncel versiyonuna sahip olmak algoritmanın çalışmamasına sebep olabilir.\n",
    "\n",
    "3. YOLO parametrelerinin ayarlanılması ve eğitilmesi:Eğitim süresinin oldukça uzun olabileceği düşünülerek parametrelerinin ayarlanmadan önce etkileri iyi analiz edilerek karar verilmelidir.YOLO dosyalarının içerisinde bulunana **train.py** dosyası eğitim için kullanılmaktadır.\n",
    "\n",
    "4. YOLO başarım performanslarının değerlendirilmesi:Model eğitimi tamamlanıldıktan sonraki aşama model başarısının matematiksel olarak ifade edilmesidir.1 adımda toplanılan ve test amacı ile ayrılan veriler eğitilmiş model ile test edilerek **Başarı Ölçütleri(Classification Metrics** başlığında bulunan ölçekler elde edilecektir.Her bir sınıfta aynı başarı oranının elde edilmesi mümkün olmayabilir.Eğer başarı durumunu 1 ile 0 arasında ifade edecek olursak ,0.5'ten daha yüksek elde edilecek bir başarı oranının kabul göreceği düşünülebilir.Ancak bu durum kullanılan veri seti,sınıf sayısı gibi birçok duruma bağlı olarak değişebilir.\n",
    "\n",
    "5. YOLO modelinin dışarıya aktarılması:Eğer elde edilen eğitilmiş modelin başarım seviyesi istenilen düzeyde ise daha sonrasında kullanılması için kaydedilmesi gerekmektedir.Bu kayıt işlemi sonrasında elde edilecek dosya içerisinde aslında her tür bir sinir hücresinin ağırlık bilgisi ve katmanların bilgisi yer alacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ce9c3-d984-4ea9-914a-7444d92a46ca",
   "metadata": {},
   "source": [
    "train.py --img --batch --epochs --data --workers --cfg --weights --name --hyp --cache\n",
    "\n",
    "**NOT: data parametresi ile verilecek yaml dosyası bir konfigurasyon dosyası olarak düşünülebilir.Bu dosya konum olarak YOLO/data klasörü içerisinde yer almalıdır.Ayrıca YOLO veri setinin konumlarının aşağıdaki gibi olması beklenilir:**\n",
    "\n",
    "train:../dataset/images/train/\n",
    "\n",
    "val:../dataset/images/val/\n",
    "\n",
    "test:../dataset/images/test/\n",
    "\n",
    "Genelde kullanılan oran %80 training(eğitim), %10 doğrulama(validation) ve %10 test amaçlıdır. Validation parçası genelde büyük veri setlerinde aşırı öğrenmenin önüne geçmek ve hesaplanana ağırlık değerlerinin optimize edilmesi için eğitim sırasında kullanılır.Ayrıca YOLO etiket dosyalarınında yine aynı klasör yapısında olmasını bekler.Yani her bir bölümün etiket dosyasının adres yolu aşağıdaki gibidir:\n",
    "\n",
    "train:../dataset/labels/train/\n",
    "\n",
    "val:../dataset/labels/val/\n",
    "\n",
    "test:../dataset/labels/test/\n",
    "\n",
    "hyp,parametresiyle verilen hiperparameter dosyası yine bir konfigürasyon dosyası olarak düşünülmelidir.Bu dosyada ise geliştirilen sinir ağının hassas parametreleri gibi düşünülebilir. Genelde direkt olarak YOLO dosyaları içerisinde yer alan **hyp.scratch.yaml** dosyası kullanılır. \n",
    "\n",
    "lr0:0.0.1  -learning rate(SGD:1e-2,Adam:1e-3)\n",
    "lrf:0.1 OneCycleLR learning rate(lr0 * lrf)\n",
    "momentum:0.937 SGD momentum/Adam beta1 \n",
    "weight_decay:0.0005 optimizer weight decay 5e-4\n",
    "warmup_epochs:3.0 (fractions ok)\n",
    "warmup_momentum:0.8 warmup initial momentum\n",
    "warmup_bias_lr:0.1 warmup initial bias lr\n",
    "box:0.05 - box loss gain\n",
    "cls: cls loss gain\n",
    "\n",
    "\n",
    "**Eğer kullanılan donanım yapınız bu eğitimi gerçekleştirmek için çok uygun değilse img,batch,cfg parametrelerinde yer alan değerler düşürülerek çalışmaya devam edilebilir.Ancak başarım performansınında etkileneceği unutulmamalıdır.**\n",
    "\n",
    "Eğitim sürecinde GPU veya CPU kullanılması mümkündür.Harici bir ekran kartı ile yapılacak eğitimin işlemciden oldukça hızlı olacağı unutulmamalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d12b40c-d65e-4f5f-898c-3809231e9b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 5)) (1.25.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 6)) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 7)) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 9)) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 10)) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 12)) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 13)) (4.65.0)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (2.13.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 20)) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 21)) (0.12.2)\n",
      "Requirement already satisfied: thop in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 37)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (23.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from requests>=2.23.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 9)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from requests>=2.23.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 9)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from requests>=2.23.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 9)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from requests>=2.23.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 9)) (2023.7.22)\n",
      "Requirement already satisfied: filelock in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tqdm>=4.41.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (1.56.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (3.4.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (2.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (0.38.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from pandas>=1.1.4->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 20)) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from pandas>=1.1.4->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 20)) (2023.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 4)) (3.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from sympy->torch>=1.7.0->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\emred\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt (line 16)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#YOLO ALGORİTMA EĞİTİLMESİ\n",
    "\n",
    "!pip install -r OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a08261f-2dab-4b73-8bec-8c20a235fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython'] not found, attempting AutoUpdate...\n",
      "Collecting gitpython\n",
      "\n",
      "  Obtaining dependency information for gitpython from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\n",
      "\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\n",
      "\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
      "\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\n",
      "     ---------------------------------------- 0.0/62.7 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/62.7 kB ? eta -:--:--\n",
      "     ------------------------ ------------- 41.0/62.7 kB 495.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.7/62.7 kB 673.0 kB/s eta 0:00:00\n",
      "\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
      "\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "\n",
      "Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\n",
      "   ---------------------------------------- 0.0/188.5 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 41.0/188.5 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 122.9/188.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 188.5/188.5 kB 1.6 MB/s eta 0:00:00\n",
      "\n",
      "Installing collected packages: smmap, gitdb, gitpython\n",
      "\n",
      "Successfully installed gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n",
      "\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  5.0s, installed 1 package: ['gitpython']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights='', cfg=OneDrive/Belgeler/OpenCV/yolov5-master/yolov5-master/models/yolov5l.yaml, data=OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/data/cats_dogs.yaml, hyp=OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\data\\hyps\\hyp.scratch-low.yaml, epochs=250, batch_size=32, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=12, project=OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\runs\\train, name=cat_dogs_model, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLOv5  2023-8-3 Python-3.9.17 torch-2.0.1+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\runs\\train', view at http://localhost:6006/\n",
      "\n",
      "Dataset not found , missing paths ['C:\\\\Users\\\\emred\\\\OneDrive\\\\Belgeler\\\\OpenCV\\\\yolov5-master\\\\yolov5-master\\\\cats_dogs\\\\val']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\train.py\", line 647, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\train.py\", line 536, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\train.py\", line 117, in train\n",
      "    data_dict = data_dict or check_dataset(data)  # check if None\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\utils\\general.py\", line 517, in check_dataset\n",
      "    raise Exception('Dataset not found \\u274c')\n",
      "Exception: Dataset not found \\u274c\n"
     ]
    }
   ],
   "source": [
    "!python OneDrive/Belgeler/OpenCV/yolov5-master/yolov5-master/train.py --img  320  --cfg  OneDrive/Belgeler/OpenCV/yolov5-master/yolov5-master/models/yolov5l.yaml --batch  32  --epochs  250  --data  OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/data/cats_dogs.yaml --weights  ''  --workers  12  --name  cat_dogs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82ee4fa-a8c1-4eb6-9aa7-3c8ab1d0e347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/runs/train/cat_dogs_model/weights/best.pt'], source=OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/test/test/, data=OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\runs\\detect, name=cat_dog_model, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  2023-8-3 Python-3.9.17 torch-2.0.1+cpu CPU\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\detect.py\", line 262, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\detect.py\", line 257, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\detect.py\", line 99, in run\n",
      "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\models\\common.py\", line 344, in __init__\n",
      "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\yolov5-master\\yolov5-master\\models\\experimental.py\", line 79, in attempt_load\n",
      "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 791, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 252, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'OneDrive\\\\Belgeler\\\\OpenCV\\\\Kitap\\\\DATA\\\\yolov5\\\\runs\\\\train\\\\cat_dogs_model\\\\weights\\\\best.pt'\n"
     ]
    }
   ],
   "source": [
    "!python OneDrive/Belgeler/OpenCV/yolov5-master/yolov5-master/detect.py  --source  OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/test/test/  --weights OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/runs/train/cat_dogs_model/weights/best.pt  --conf 0.25  --name cat_dog_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61076d0c-21b2-4285-b95b-7220a848cb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/runs/train/cat_dogs_model/weights/best.pt'], source=OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/test/test/, data=OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\runs\\detect, name=cat_dogs_model, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  2022-1-20 torch 2.0.1+cpu CPU\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\detect.py\", line 257, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\detect.py\", line 252, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\detect.py\", line 92, in run\n",
      "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\models\\common.py\", line 308, in __init__\n",
      "    model = attempt_load(weights if isinstance(weights, list) else w, map_location=device)\n",
      "  File \"C:\\Users\\emred\\OneDrive\\Belgeler\\OpenCV\\Kitap\\DATA\\yolov5\\models\\experimental.py\", line 96, in attempt_load\n",
      "    ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 791, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"C:\\Users\\emred\\anaconda3\\envs\\yoloEnv\\lib\\site-packages\\torch\\serialization.py\", line 252, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'OneDrive\\\\Belgeler\\\\OpenCV\\\\Kitap\\\\DATA\\\\yolov5\\\\runs\\\\train\\\\cat_dogs_model\\\\weights\\\\best.pt'\n"
     ]
    }
   ],
   "source": [
    "!python OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/detect.py  --source OneDrive/Belgeler/OpenCV/Kitap/DATA/cats_dosg_3/test/test/  --weights OneDrive/Belgeler/OpenCV/Kitap/DATA/yolov5/runs/train/cat_dogs_model/weights/best.pt --conf 0.25  --name cat_dogs_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
